# 正誤表
| 箇所          | 内容                                                        | 修正                             | 備考                         |
| :------------: | --------------------------------------------------------- | ------------------------------ | -------------------------- |
| p. 131, eq. (6.6)  | $\log{y_k^t}({\bf x}_n^t;{\bf w})$   | $\log{y_k^t}(({\bf x}_n^i)_{i=1}^t;{\bf w})$  |   |


# グレーゾーンな内容
| 箇所          | 内容                                                        | 修正                             | 備考                         |
| :------------: | --------------------------------------------------------- | ------------------------------ | -------------------------- |
| p. 100, Sec. 5.7.2, para. 1 | 2 以上のストライドを持つプーリング層や畳み込み層 | 2 以上のストライドを持つ畳み込み層やプーリング層  |  |
| p. 108, para.c | $\phi$ を，式 (5.24) のように選んでいれば  |   | 最近傍サンプリングカーネルの微分はほとんどいたるところゼロなので訓練には使えないと思います[^1]．好意的に解釈すると，サンプリングカーネルの式が適当にどれか一つ式 (5.24) のように定めていれば，と読めば良い気はしますが．  |
| p. 128, para.c | RNN は（少なくとも理論上）過去のすべての入力から１つの出力への写像を表します． |   | 以下の ”RNN が入力の履歴の写像にならない場合について” 参照 |
| p. 137, Sec. 6.4.1  |   |   | このあと出てくる”自己回帰モデル”はほとんど RNN で作られていて，非線形なものだと思うのでここで線形自己回帰モデルの式を出してる理由がかなり謎です．  |

## RNN が入力の履歴の写像にならない場合について
RNNが記憶を失わないようなとき，RNNの出力は入力の履歴の写像になりません．記憶を失わないRNNは初期条件の影響が残り続けるので，入力の履歴だけだと出力の候補が複数存在しえます[^2]．これは写像の定義を満たしません（写像とは左全域的 (left-total) かつ多対一 (many to one) の関係 (relation: 数学用語です) のことです）．これは重箱の隅をつついてるわけではなくて，リザバーのESP（入力の履歴だけに依存し，初期条件に依存しない性質）がなぜ要請されるのか，なんかを考えると工学的に重要な事柄です．出力が入力の履歴だけでなく初期条件にも依存してしまうということは，そのRNNをうまく働かせるために初期条件をうまく準備しなければならないということです．この状況は系の細部を外部から調節することが難しい NN の物理実装と非常に相性が悪いです．リザバーに記憶を失う性質と（非常にマイルドな条件下で）等価な ESP を要請する主な理由のひとつはここにあります．

[^1]: 適当な意味での劣微分を採用すればすべての点でゼロにすることも正当化できそうに思います．凸関数じゃないので広く流通してるような劣微分の概念は適用できなさそうですが．
[^2]: 入力が恒等的にゼロで全く記憶をうしなわないLSTMメモリセルなんかを考えれば良いでしょう．このメモリセルの状態は初期条件のままです．出力はこの初期条件に依存して変化します．入力の履歴の集合は単一要素（恒等的にゼロ）からなりますが出力の候補が複数存在してしまっています．

